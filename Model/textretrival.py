# -*- coding: utf-8 -*-
"""TextRetrival.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-6taugwiRAbY7sdXBEzsqcNpj6jhMoij
"""

!pip install -U datasets huggingface_hub fsspec

from datasets import load_dataset

dataset = load_dataset('ms_marco','v1.1')

subset = dataset['test']
subset

subset['passages'][0]

corpus = []

for sample in subset:
  query_type = sample['query_type']
  if query_type != 'entity':
    continue
  passage_dict = sample['passages']
  passage_text_lst = passage_dict['passage_text']

  corpus+=passage_text_lst
len(corpus)

corpus[0]

import nltk
nltk.download('stopwords')

def lowercase(text):
  return text.lower()

import string

remove_chars = string.punctuation
def remove_punctuation(text):
  for char in remove_chars:
    text = text.replace(char, '')

  return text

from nltk.corpus import stopwords

stopwords_lst = stopwords.words('english')

def remove_stopwords(text):
  tokens = tokenize(text)
  non_stopword_lst = [token for token in tokens if token not in stopwords_lst]
  new_text = ' '.join(non_stopword_lst)

  return new_text

from nltk.stem import PorterStemmer

steamer = PorterStemmer()

def steaming(text):
  tokens = tokenize(text)
  stemmed_lst = [steamer.stem(token) for token in tokens]

  new_text = ' '.join(stemmed_lst)
  return new_text

def text_normalize(text):
  text = lowercase(text)
  text = remove_punctuation(text)
  text = remove_stopwords(text)
  text = steaming(text)

  return text

def tokenize(text):
  return text.split()

def create_dictionary(corpus):
  dictionary = []
  for doc in corpus:
    normalized_doc = text_normalize(doc)
    tokens = tokenize(doc)
    for token in tokens:
      if token not in dictionary:
        dictionary.append(token)

  return dictionary

dictionary = create_dictionary(corpus)

def vectorize(text,dictionary):
  word_count_dict = {word:0 for word in dictionary}
  normalized_text = text_normalize(text)
  tokens = tokenize(normalized_text)
  for token in tokens:
    try:
      word_count_dict[token]+=1
    except:
      pass
  vector = list(word_count_dict.values())
  return word_count_dict

def create_doc_term_matrix(corpus,dictionary):
  doc_term_matrix = {}
  for idx,doc in enumerate(corpus):
    vector = vectorize(doc,dictionary)
    doc_term_matrix[(doc,idx)] = vector

  return doc_term_matrix

doc_term_matrix = create_doc_term_matrix(corpus,dictionary)

from scipy import spatial

def similarity(a,b):
  return 1 - spatial.distance.cosine(a,b)

def ranking(query,dictionary,doc_term_matrix):
  query_vec = vectorize(query,dictionary)
  scores = []
  for doc_info,doc_vec in doc_term_matrix.items():
    sim = similarity(query_vec,doc_vec)
    scores.append((sim,doc_info))
  scores.sort(reverse=True)
  return scores

query_lst = ['What is the official language of Canada?']
top_k = 10

for query in query_lst:
  scores = ranking(query,dictionary,doc_term_matrix)
  print(f'Query: {query}')
  for idx in range(top_k):
    doc_score = scores[idx][0]
    doc_content = scores[idx][1][0]

    print(f'Top: {idx+1}, Score: {doc_content:.4f}')
    print(doc_content)
    print('\n')

